{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Import Dataset"
      ],
      "metadata": {
        "id": "Ks4eHjYg5Aaz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkGhCRUC45YE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip',\n",
        "                 compression='zip',\n",
        "                 low_memory=False)\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###first few rows of dataset."
      ],
      "metadata": {
        "id": "ncxqnj9W5SQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Z2R92yiP5NtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###remove 'qid' from the dataset."
      ],
      "metadata": {
        "id": "mIgkAVxp5ZQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('qid', axis=1)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "PyswQr1i5Uqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###split the dataset"
      ],
      "metadata": {
        "id": "kI6fmOFy5nOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First split: 80% train, 20% temporary\n",
        "train_sentences, temp_sentences, train_labels, temp_labels = train_test_split(\n",
        "    df['question_text'].to_numpy(),\n",
        "    df['target'].to_numpy(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Second split: 10% validation, 10% test (from the 20% temporary set)\n",
        "val_sentences, test_sentences, val_labels, test_labels = train_test_split(\n",
        "    temp_sentences,\n",
        "    temp_labels,\n",
        "    test_size=0.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(train_sentences)}\")\n",
        "print(f\"Validation set size: {len(val_sentences)}\")\n",
        "print(f\"Test set size: {len(test_sentences)}\")"
      ],
      "metadata": {
        "id": "fme09v9c5tmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokenization + Padding"
      ],
      "metadata": {
        "id": "0TCjaMgY5x0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Parameters for padding and OOV tokens\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "vocab_size = 10000\n",
        "max_length = 32\n",
        "\n",
        "# Initialize the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "\n",
        "# Generate the word index dictionary\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generate and pad the training sequences\n",
        "training_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Generate and pad the testing sequences\n",
        "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Convert the labels lists into numpy arrays\n",
        "training_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)"
      ],
      "metadata": {
        "id": "QMlmn-Kw54rD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example for Tokenization"
      ],
      "metadata": {
        "id": "SV4FXd9n_l7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sequence = tokenizer.texts_to_sequences([\"Do you have an adopted dog, how would you encourage people to adopt and not shop?\"])\n",
        "example_sequence"
      ],
      "metadata": {
        "id": "xq8jd0Qj_vAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example for padding"
      ],
      "metadata": {
        "id": "ngMAgxGpAJou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_padded= pad_sequences(example_sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "example_padded"
      ],
      "metadata": {
        "id": "AHH7WaQI7cUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create the Model"
      ],
      "metadata": {
        "id": "GwJ7mKrY6JB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Output dimensions of the Embedding layer\n",
        "embedding_dim = 16\n",
        "vocab_size = 10000\n",
        "max_length = 32\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.build(input_shape=(None, max_length))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vRE2lrJp6ByL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Execute following code cell to get understanding about GlobalAveragePooling1D() layer"
      ],
      "metadata": {
        "id": "Qn8dVPlB6RL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a GlobalAveragePooling1D layer\n",
        "gap1d_layer = tf.keras.layers.GlobalAveragePooling1D()\n",
        "\n",
        "# Define sample array\n",
        "sample_array = np.array([[[20,4],[3,9],[10,10]]])\n",
        "\n",
        "# Print shape and contents of sample array\n",
        "print(f'shape of sample_array = {sample_array.shape}')\n",
        "print(f'sample array: {sample_array}')\n",
        "\n",
        "# put sample array as input ot GlobalAveragePooling1D layer\n",
        "output = gap1d_layer(sample_array)\n",
        "\n",
        "# Print shape and contents of the GlobalAveragePooling1D output array\n",
        "print(f'output shape of gap1d_layer: {output.shape}')\n",
        "print(f'output array of gap1d_layer: {output.numpy()}')"
      ],
      "metadata": {
        "id": "xZhFPDBU6MU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Compile the model"
      ],
      "metadata": {
        "id": "AT8MQngl6fsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(training_padded, training_labels, epochs=30, validation_data=(val_padded, val_labels), verbose=2)"
      ],
      "metadata": {
        "id": "mRDWikMW6dSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plot Accuracy and Loss of the model with each epoch"
      ],
      "metadata": {
        "id": "2fAGZUX46jmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot utility\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "# Plot the accuracy and loss\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "QVslIByj6jVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get the index-word dictionary"
      ],
      "metadata": {
        "id": "eEv_xFNUHP37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the index-word dictionary\n",
        "reverse_word_index = tokenizer.index_word\n",
        "\n",
        "# Get the embedding layer from the model (i.e. first layer)\n",
        "embedding_layer = model.layers[0]\n",
        "\n",
        "# Get the weights of the embedding layer\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "# Print the shape. Expected is (vocab_size, embedding_dim)\n",
        "print(embedding_weights.shape)"
      ],
      "metadata": {
        "id": "9lY4J4okHOzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "# Open writeable files\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
        "for word_num in range(1, vocab_size):\n",
        "\n",
        "  # Get the word associated at the current index\n",
        "  word_name = reverse_word_index[word_num]\n",
        "\n",
        "  # Get the embedding weights associated with the current index\n",
        "  word_embedding = embedding_weights[word_num]\n",
        "\n",
        "  # Write the word name\n",
        "  out_m.write(word_name + \"\\n\")\n",
        "\n",
        "  # Write the word embedding\n",
        "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
        "\n",
        "# Close the files\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "metadata": {
        "id": "BUcLFAVTHo17"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###To download, these files into your local machine,"
      ],
      "metadata": {
        "id": "r8muNSLQ625T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import files utilities in Colab\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "# Download the files\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "metadata": {
        "id": "LKgKZfUm666K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get Token for any Word"
      ],
      "metadata": {
        "id": "TQZvhRk5FT7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input a word\n",
        "word = input(\"Enter a word: \")\n",
        "\n",
        "# Get the index of the word\n",
        "word_idx = tokenizer.word_index.get(word)\n",
        "\n",
        "if word_idx:\n",
        "  print(f\"Token for '{word}': {word_idx}\")\n",
        "else:\n",
        "  print(f\"The word '{word}' is not in the vocabulary.\")\n"
      ],
      "metadata": {
        "id": "FNAP6hyHFRC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get the word embeddings for any word"
      ],
      "metadata": {
        "id": "UlP7mQ5oFM7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embedding layer from the model\n",
        "embedding_layer = model.layers[0]\n",
        "\n",
        "# Get the weights of the embedding layer\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "# Get the word index\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Input a word\n",
        "word = input(\"Enter a word: \")\n",
        "\n",
        "# Get the index of the word\n",
        "word_idx = word_index.get(word)\n",
        "\n",
        "if word_idx:\n",
        "  # Get the embedding vector for the word\n",
        "  embedding_vector = embedding_weights[word_idx]\n",
        "  print(f\"Embedding vector for '{word}': {embedding_vector}\")\n",
        "else:\n",
        "  print(f\"The word '{word}' is not in the vocabulary.\")\n"
      ],
      "metadata": {
        "id": "Cho4jPtpFLnj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}